{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 只做单模型，其他的内容，等等再说，现在把单模型的表现提升上去再说,这份code的目标只是为了提升模型的分类效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Leo Mao\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import collections\n",
    "import time\n",
    "import random\n",
    "\n",
    "import model_ml as mm\n",
    "import feat_engineering as fe\n",
    "import feat_selection as fs\n",
    "import model_tunning as mt\n",
    "from param_config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv(config.original_train_data_path)\n",
    "dfPred = pd.read_csv(config.original_pred_data_path)\n",
    "predictors = dfPred.columns.tolist()[4:]\n",
    "check_missing = ['PartI_1','PartII_1','PartIII_1','PartIV_1','PartV_1','PartVI_1']\n",
    "\n",
    "###清理异常Y值\n",
    "#异常大值\n",
    "dfTrain = dfTrain.loc[dfTrain['Y']<dfTrain['Y'].max()]\n",
    "\n",
    "#性别空\n",
    "dfTrain = dfTrain.loc[~dfTrain['sex'].isnull()]\n",
    "\n",
    "dfTrain = dfTrain.reset_index(drop=True)\n",
    "\n",
    "saveY = dfTrain['Y'].tolist()\n",
    "\n",
    "dfTrain.loc[dfTrain['Y']>dfTrain['Y'].quantile(0.9),'Y'] = 1\n",
    "dfTrain.loc[dfTrain['Y']!=1,'Y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FeatAll(train,pred,rankMinus=False,missing=False,missing_step2=False):\n",
    "    dfAll = pd.concat([train,pred])\n",
    "    dfAll = dfAll.reset_index(drop=True)\n",
    "    dfAll['date'] = (pd.to_datetime(dfAll['date']) - parse('2017-10-09')).dt.days\n",
    "    \n",
    "    ###RF填补缺失值\n",
    "    if missing:\n",
    "        dfAll = fe.fillna_RF(dfAll,predictors,True)\n",
    "\n",
    "    \n",
    "    ###尝试处理异常值\n",
    "    '''for var in predictors:\n",
    "        dfAll.loc[dfAll[var]>=dfAll[var].quantile(0.99)*3,var] = dfAll[var].quantile(0.99)*3'''\n",
    "        #dfAll.loc[dfAll[var]>=df[var].quantile(0.99)*1.5,var] = np.nan\n",
    "    ###根据年龄性别去做分类，鉴别不同人的百分比\n",
    "    dfAll['ageBin'] = 0\n",
    "    ageBin = [20,30,40,50,60]\n",
    "    for i in range(len(ageBin)):\n",
    "        dfAll.loc[dfAll['age']>ageBin[i],'ageBin'] = i+1\n",
    "    dfAll['sexAge'] = dfAll[['sex','ageBin']].apply(lambda x: x[1]*2+x[0])\n",
    "    dfAllPcentBySexAge = fe.pcent_by_other_col(dfAll,{'sex':predictors,'ageBin':predictors,'sexAge':predictors},['ID'])\n",
    "    del dfAll['age']\n",
    "    del dfAll['sexAge']\n",
    "    if rankMinus:\n",
    "        try:\n",
    "            for i in ['_sex_','_ageBin_','_sexAge']:\n",
    "                dfrankMinusTmp = pd.read_csv('../../Cache/rankMinus%s.csv'%i[:-1])\n",
    "                if i == '_sex_':\n",
    "                    dfrankMinus = dfrankMinusTmp.copy()\n",
    "                else:\n",
    "                    dfrankMinus = dfrankMinus.merge(dfrankMinusTmp,'inner','ID')\n",
    "        except:\n",
    "            for i in ['_sex_','_ageBin_','_sexAge']:\n",
    "                varList = [var for var in dfAllPcentBySexAge.columns if i in var]\n",
    "                dfrankMinusTmp = fe.var_minus(dfAllPcentBySexAge,varList,['ID'],0)\n",
    "                dfrankMinusTmp.to_csv('../../Cache/rankMinus%s.csv'%i[:-1],index=False)\n",
    "                if i == '_sex_':\n",
    "                    dfrankMinus = dfrankMinusTmp.copy()\n",
    "                else:\n",
    "                    dfrankMinus = dfrankMinus.merge(dfrankMinusTmp,'inner','ID')  \n",
    "\n",
    "    \n",
    "    ###目前不想使用日期，觉得用处不大\n",
    "    del dfAll['date']\n",
    "\n",
    "    #for minus in\n",
    "    \n",
    "    for plus in [['PartII_1','PartII_2'],['PartIII_1','PartIII_2','PartIII_3']]:\n",
    "        tmpVar =plus[0]\n",
    "        tmpValue = dfAll[plus[0]].values\n",
    "        for var in plus[1:]:\n",
    "            tmpVar = tmpVar +'_plus_' + var\n",
    "            tmpValue = tmpValue + dfAll[var].values\n",
    "        dfAll[tmpVar] = tmpValue\n",
    "    \n",
    "    dfAll['PartI_9'] = dfAll['PartI_5']-dfAll['PartI_6']-dfAll['PartI_7']\n",
    "    dfAll['PartII_5'] = dfAll['PartII_2']-dfAll['PartII_3']-dfAll['PartII_4']\n",
    "    \n",
    "    for ratio in [['PartII_1','PartII_2'],['PartI_6','PartI_5'],['PartI_7','PartI_5'],['PartII_3','PartII_2'],['PartII_4','PartII_2'],['PartV_2','PartV_1'],['PartII_1','PartII_2']]:\n",
    "        dfAll[ratio[0]+'_divided_'+ratio[1]] = dfAll[ratio[0]]/dfAll[ratio[1]]\n",
    "        \n",
    "    for multiply in [['PartI_1','PartI_5'],['PartI_2','PartI_5'],['PartI_3','PartI_5'],['PartI_4','PartI_5'],['PartVI_1','PartVI_2'],['PartV_2','PartV_5'],['PartV_2','PartV_7'],['PartV_1','PartV_9'],['PartV_1','PartV_10'],['PartV_1','PartV_11'],['PartV_1','PartV_12'],['PartV_1','PartV_13']]:\n",
    "        dfAll[multiply[0]+'_multiply_'+multiply[1]] = dfAll[multiply[0]]*dfAll[multiply[1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        dfAll = dfAll.merge(dfAllPcentBySexAge,'inner','ID')\n",
    "    except:\n",
    "        print('No Pcent by sex and age')\n",
    "        \n",
    "    try:\n",
    "        dfAll = dfAll.merge(dfrankMinus,'inner','ID')\n",
    "    except:\n",
    "        print('No Rank Minus')    \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    ###填补缺失值\n",
    "    if missing_step2:\n",
    "        dfAll =dfAll.fillna(dfAll.median())\n",
    "    \n",
    "    dfTrain = dfAll.loc[dfAll['ID'].isin(train['ID'])]\n",
    "    dfPred = dfAll.loc[dfAll['ID'].isin(pred['ID'])]\n",
    "    \n",
    "    return dfTrain,dfPred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo Mao\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "dfTrain,dfPred = FeatAll(dfTrain,dfPred,True,True,True)\n",
    "predictors = dfPred.columns.tolist()\n",
    "predictors.remove('ID')\n",
    "predictors.remove('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 0, use 203 seconds\n",
      "finish 1, use 204 seconds\n",
      "finish 2, use 203 seconds\n",
      "finish 3, use 203 seconds\n",
      "finish 4, use 203 seconds\n",
      "finish 5, use 203 seconds\n",
      "finish 6, use 204 seconds\n",
      "finish 7, use 203 seconds\n",
      "finish 8, use 203 seconds\n",
      "finish 9, use 203 seconds\n",
      "finish 10, use 203 seconds\n",
      "finish 11, use 203 seconds\n",
      "finish 12, use 203 seconds\n",
      "finish 13, use 203 seconds\n",
      "finish 14, use 203 seconds\n",
      "finish 15, use 203 seconds\n",
      "finish 16, use 203 seconds\n",
      "finish 17, use 203 seconds\n",
      "finish 18, use 203 seconds\n",
      "finish 19, use 210 seconds\n",
      "finish 20, use 209 seconds\n",
      "finish 21, use 205 seconds\n",
      "finish 22, use 204 seconds\n",
      "finish 23, use 204 seconds\n",
      "finish 24, use 203 seconds\n",
      "finish 25, use 203 seconds\n",
      "finish 26, use 203 seconds\n",
      "finish 27, use 203 seconds\n",
      "finish 28, use 203 seconds\n",
      "finish 29, use 204 seconds\n",
      "finish 30, use 204 seconds\n",
      "finish 31, use 203 seconds\n",
      "finish 32, use 203 seconds\n",
      "finish 33, use 204 seconds\n",
      "finish 34, use 204 seconds\n",
      "finish 35, use 203 seconds\n",
      "finish 36, use 203 seconds\n",
      "finish 37, use 203 seconds\n",
      "finish 38, use 203 seconds\n",
      "finish 39, use 203 seconds\n",
      "finish 40, use 203 seconds\n",
      "finish 41, use 203 seconds\n",
      "finish 42, use 204 seconds\n",
      "finish 43, use 203 seconds\n",
      "finish 44, use 204 seconds\n",
      "finish 45, use 203 seconds\n",
      "finish 46, use 203 seconds\n",
      "finish 47, use 203 seconds\n",
      "finish 48, use 203 seconds\n",
      "finish 49, use 203 seconds\n",
      "finish 50, use 203 seconds\n",
      "finish 51, use 203 seconds\n",
      "finish 52, use 203 seconds\n",
      "finish 53, use 203 seconds\n",
      "finish 54, use 204 seconds\n",
      "finish 55, use 203 seconds\n",
      "finish 56, use 203 seconds\n",
      "finish 57, use 203 seconds\n",
      "finish 58, use 203 seconds\n",
      "finish 59, use 203 seconds\n",
      "finish 60, use 203 seconds\n",
      "finish 61, use 203 seconds\n",
      "finish 62, use 203 seconds\n",
      "finish 63, use 203 seconds\n",
      "finish 64, use 203 seconds\n",
      "finish 65, use 203 seconds\n",
      "finish 66, use 203 seconds\n",
      "finish 67, use 204 seconds\n",
      "finish 68, use 203 seconds\n",
      "finish 69, use 203 seconds\n",
      "finish 70, use 203 seconds\n",
      "finish 71, use 202 seconds\n",
      "finish 72, use 203 seconds\n",
      "finish 73, use 203 seconds\n",
      "finish 74, use 203 seconds\n",
      "finish 75, use 203 seconds\n",
      "finish 76, use 203 seconds\n",
      "finish 77, use 204 seconds\n",
      "finish 78, use 203 seconds\n",
      "finish 79, use 204 seconds\n",
      "finish 80, use 203 seconds\n",
      "finish 81, use 203 seconds\n",
      "finish 82, use 203 seconds\n",
      "finish 83, use 203 seconds\n",
      "finish 84, use 203 seconds\n",
      "finish 85, use 203 seconds\n",
      "finish 86, use 205 seconds\n",
      "finish 87, use 204 seconds\n",
      "finish 88, use 204 seconds\n",
      "finish 89, use 204 seconds\n",
      "finish 90, use 206 seconds\n",
      "finish 91, use 205 seconds\n",
      "finish 92, use 205 seconds\n",
      "finish 93, use 205 seconds\n",
      "finish 94, use 204 seconds\n",
      "finish 95, use 205 seconds\n",
      "finish 96, use 204 seconds\n",
      "finish 97, use 204 seconds\n",
      "finish 98, use 205 seconds\n",
      "finish 99, use 205 seconds\n",
      "finish 100, use 205 seconds\n",
      "finish 101, use 294 seconds\n",
      "finish 102, use 204 seconds\n",
      "finish 103, use 205 seconds\n",
      "finish 104, use 204 seconds\n",
      "finish 105, use 205 seconds\n",
      "finish 106, use 205 seconds\n",
      "finish 107, use 205 seconds\n",
      "finish 108, use 204 seconds\n",
      "finish 109, use 205 seconds\n",
      "finish 110, use 204 seconds\n",
      "finish 111, use 205 seconds\n",
      "finish 112, use 206 seconds\n",
      "finish 113, use 205 seconds\n",
      "finish 114, use 206 seconds\n",
      "finish 115, use 205 seconds\n",
      "finish 116, use 204 seconds\n",
      "finish 117, use 205 seconds\n",
      "finish 118, use 205 seconds\n",
      "finish 119, use 205 seconds\n",
      "finish 120, use 205 seconds\n",
      "finish 121, use 206 seconds\n",
      "finish 122, use 204 seconds\n",
      "finish 123, use 204 seconds\n",
      "finish 124, use 204 seconds\n",
      "finish 125, use 205 seconds\n",
      "finish 126, use 208 seconds\n",
      "finish 127, use 205 seconds\n",
      "finish 128, use 205 seconds\n",
      "finish 129, use 205 seconds\n",
      "finish 130, use 205 seconds\n",
      "finish 131, use 204 seconds\n",
      "finish 132, use 205 seconds\n",
      "finish 133, use 205 seconds\n",
      "finish 134, use 205 seconds\n",
      "finish 135, use 204 seconds\n",
      "finish 136, use 205 seconds\n",
      "finish 137, use 205 seconds\n",
      "finish 138, use 204 seconds\n",
      "finish 139, use 205 seconds\n",
      "finish 140, use 205 seconds\n",
      "finish 141, use 204 seconds\n",
      "finish 142, use 204 seconds\n",
      "finish 143, use 205 seconds\n",
      "finish 144, use 204 seconds\n",
      "finish 145, use 205 seconds\n",
      "finish 146, use 204 seconds\n",
      "finish 147, use 204 seconds\n",
      "finish 148, use 206 seconds\n",
      "finish 149, use 218 seconds\n"
     ]
    }
   ],
   "source": [
    "predictorsRF =[]\n",
    "for i in range(150):\n",
    "    t1 = datetime.datetime.now()\n",
    "    predictorsRF += fs.RF_selection(dfTrain,predictors,int(random.random()*10000))[:1000]\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(\"finish %d, use %d seconds\"%(i,(t2-t1).seconds))\n",
    "count = collections.Counter(predictorsRF)\n",
    "dfpredictorsRF = pd.DataFrame(count,index=['count']).T\n",
    "dfpredictorsRF.to_csv('../../Cache/Feat.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfpredictorsRF.to_csv('../../Cache/Feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testResult = dfTrain[['ID','Y']]\n",
    "predResult = dfPred[['ID','Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "early_stop=50\n",
    "params = {'max_depth':5, 'eta':0.01, 'silent':0,'objective':'binary:logistic','lambda':1,'subsample':0.8,'colsample_bytree':0.8,'eval_metric':'logloss'}\n",
    "repeatRound =10\n",
    "selectNum = 150\n",
    "modelPredictors = predictors[:10]\n",
    "for i in range(repeatRound):\n",
    "    test_result,result,imp = mm.xgb_kfold(dfTrain,dfPred,tmpPredictor,n_splits,early_stop = early_stop,params=params,seed=int(random.random()*10000))\n",
    "    for j in range(1,n_splits+1):\n",
    "        imp['imp_fold%d'%j] = imp['imp_fold%d'%j]/imp['imp_fold%d'%j].sum()\n",
    "    imp['sum_imp%d'%i] = imp[['imp_fold%d'%k for k in range(1,n_splits+1)]].sum(axis=1)\n",
    "    if i ==0:\n",
    "        importance = imp[['variable','sum_imp%d'%i]]\n",
    "    else:\n",
    "        importance = importance.merge(imp[['variable','sum_imp%d'%i]],'inner','variable')\n",
    "\n",
    "importance['sum_total'] = importance[['sum_imp%d'%k for k in range(repeatRound)]].sum(axis=1)\n",
    "tmpPredictor = importance.sort_values('sum_total',ascending=False)['variable'].values.tolist()[:selectNum] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###xgb\n",
    "n_splits=5\n",
    "early_stop=50\n",
    "repeatRound =2\n",
    "selectNum = 150\n",
    "modelPredictors = tmpPredictor.copy()\n",
    "for i in range(repeatRound):\n",
    "    test_result,result,imp = mm.xgb_kfold(dfTrain,dfPred,modelPredictors,n_splits,early_stop = early_stop,params=params)\n",
    "    result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "    if i ==0:\n",
    "        tmpTest_result = test_result[['ID','score']]\n",
    "        tmpResult = result[['ID','score']]\n",
    "    else:\n",
    "        tmpTest_result = tmpTest_result.merge(test_result[['ID','score']],'inner','ID')\n",
    "        tmpResult = tmpResult.merge(result[['ID','score']],'inner','ID')\n",
    "    tmpTest_result.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "    tmpResult.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "testResult['cateScoreXGB'] = tmpTest_result[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values\n",
    "predResult['cateScoreXGB'] = tmpResult[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RF\n",
    "n_splits=5\n",
    "early_stop=50\n",
    "repeatRound =2\n",
    "selectNum = 150\n",
    "modelPredictors = predictors[:10]\n",
    "for i in range(repeatRound):\n",
    "    test_result,result,imp = mm.rf_kfold(dfTrain,dfPred,modelPredictors,n_splits,seed=int(random.random()*10000))\n",
    "    result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "    if i ==0:\n",
    "        tmpTest_result = test_result[['ID','score']]\n",
    "        tmpResult = result[['ID','score']]\n",
    "    else:\n",
    "        tmpTest_result = tmpTest_result.merge(test_result[['ID','score']],'inner','ID')\n",
    "        tmpResult = tmpResult.merge(result[['ID','score']],'inner','ID')\n",
    "    tmpTest_result.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "    tmpResult.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "testResult['cateScoreRF'] = tmpTest_result[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values\n",
    "predResult['cateScoreRF'] = tmpResult[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###catboost\n",
    "n_splits=5\n",
    "early_stop=50\n",
    "repeatRound =2\n",
    "selectNum = 150\n",
    "modelPredictors = predictors[:10]\n",
    "for i in range(repeatRound):\n",
    "    test_result,result,imp = mm.catboost_kfold(dfTrain,dfPred,modelPredictors,n_splits,early_stop=50,seed=int(random.random()*10000))\n",
    "    result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "    if i ==0:\n",
    "        tmpTest_result = test_result[['ID','score']]\n",
    "        tmpResult = result[['ID','score']]\n",
    "    else:\n",
    "        tmpTest_result = tmpTest_result.merge(test_result[['ID','score']],'inner','ID')\n",
    "        tmpResult = tmpResult.merge(result[['ID','score']],'inner','ID')\n",
    "    tmpTest_result.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "    tmpResult.rename(columns={'score':'score_%d'%i},inplace=True)\n",
    "testResult['cateScoreCat'] = tmpTest_result[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values\n",
    "predResult['cateScoreCat'] = tmpResult[['score_%d'%i for i in range(repeatRound)]].mean(axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectNum = 150\n",
    "for i in range(1,n_splits+1):\n",
    "    imp['imp_fold%d'%i] = imp['imp_fold%d'%i]/imp['imp_fold%d'%i].sum()\n",
    "imp['sum_imp'] = imp[['imp_fold%d'%i for i in range(1,n_splits+1)]].sum(axis=1)\n",
    "\n",
    "tmpPredictor = imp.sort_values('sum_imp',ascending=False)['variable'].values.tolist()[:selectNum]\n",
    "imp.sort_values('sum_imp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result,result,imp_selected = mm.xgb_kfold(dfTrain,dfPred,tmpPredictor,n_splits,early_stop = early_stop,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mt.evaluate_performance(test_result['target'],test_result['score'])\n",
    "imp.sort_values('sum_imp',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##将分类打分放入原始数据\n",
    "dfTrain['scoreBinary'] = test_result['score']\n",
    "result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "dfPred['scoreBinary'] = result['score'].tolist()\n",
    "\n",
    "predictorReg = predictors.copy()\n",
    "predictorReg.append('scoreBinary')\n",
    "\n",
    "dfTrain['Y'] = saveY\n",
    "\n",
    "##将分数以rank形式保存一份\n",
    "'''dfTrain['scoreBinaryPcent'] = dfTrain['scoreBinary'].rank()/float(dfTrain.shape[0])\n",
    "dfPred['scoreBinaryPcent'] = dfPred['scoreBinary'].rank()/float(dfPred.shape[0])\n",
    "predictorReg.append('scoreBinaryPcent')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dfTrain['scoreBinary'].mean())\n",
    "print(dfTrain['scoreBinary'].max())\n",
    "print(dfTrain['scoreBinary'].min())\n",
    "print('--------------')\n",
    "print(dfPred['scoreBinary'].mean())\n",
    "print(dfPred['scoreBinary'].max())\n",
    "print(dfPred['scoreBinary'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictorsRF = fs.RF_selection(dfTrain,predictorReg)\n",
    "modelPredictors = predictorsRF[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "early_stop=50\n",
    "params = {'max_depth':5, 'eta':0.01, 'silent':0,'objective':'reg:linear','lambda':1,'subsample':0.8,'colsample_bytree':0.8}\n",
    "test_result,result,imp = mm.xgb_kfold(dfTrain,dfPred,modelPredictors,n_splits,early_stop=early_stop,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectNum = 100\n",
    "for i in range(1,n_splits+1):\n",
    "    imp['imp_fold%d'%i] = imp['imp_fold%d'%i]/imp['imp_fold%d'%i].sum()\n",
    "imp['sum_imp'] = imp[['imp_fold%d'%i for i in range(1,n_splits+1)]].sum(axis=1)\n",
    "\n",
    "tmpPredictor = imp.sort_values('sum_imp',ascending=False)['variable'].values.tolist()[0:selectNum+1]\n",
    "imp.sort_values('sum_imp',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result,result,imp_selected = mm.xgb_kfold(dfTrain,dfPred,tmpPredictor,n_splits,early_stop=early_stop,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,n_splits+1):\n",
    "    imp_selected['imp_fold%d'%i] = imp_selected['imp_fold%d'%i]/imp_selected['imp_fold%d'%i].sum()\n",
    "imp_selected['sum_imp'] = imp_selected[['imp_fold%d'%i for i in range(1,n_splits+1)]].sum(axis=1)\n",
    "imp_selected.sort_values('sum_imp',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "print((result['score']>6.8).sum())\n",
    "print(result['score'].min())\n",
    "print(result['score'].max())\n",
    "print(result['score'].mean())\n",
    "print(result['score'].median())\n",
    "print(\"-------------------------\")\n",
    "print(result.iloc[938,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "other_note ='_classfier_first'\n",
    "result['score']=result[['Score_%d'%i for i in range(1,n_splits+1)]].mean(axis=1)\n",
    "submit = result[['ID','score']]\n",
    "today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "result.to_csv('../../Submission/result/result_%s'%today+other_note+'.csv',index=False)\n",
    "submit['score'].to_csv('../../Submission/submit_%s'%today+other_note+'.csv',header=False,index=False)\n",
    "test_result.to_csv('../../Submission/test/test_result_%s'%today+other_note+'.csv',index=False)\n",
    "imp.to_csv('../../Submission/imp/importance_%s'%today+other_note+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
